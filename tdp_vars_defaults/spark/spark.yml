# Copyright 2022 TOSIT.IO
# SPDX-License-Identifier: Apache-2.0

---
# Spark version
spark_version: spark
spark_release: spark-2.3.5-TDP-0.1.0-SNAPSHOT-bin-tdp
spark_dist_file: "{{ spark_release }}.tgz"
spark_hbase_dist_file: hbase-spark-2.3.5-1.0.0-TDP-0.1.0-SNAPSHOT.jar

# Spark HBase Connector
spark_hbase_connector_enable: true

# Spark users and group
spark_user: spark
hadoop_group: hadoop

# Spark installation directory
spark_root_dir: /opt/tdp
spark_install_dir: "{{ spark_root_dir }}/spark"

# Spark configuration directories
spark_conf_dir: /etc/spark
spark_client_conf_dir: "{{ spark_conf_dir }}/conf"
spark_hs_conf_dir: "{{ spark_conf_dir }}/conf.hs"

# HBase configuration directories
hbase_conf_dir: /etc/hbase
hbase_client_conf_dir: "{{ hbase_conf_dir }}/conf"
hbase_install_dir: "{{ spark_root_dir }}/hbase"

# Spark pid directories
spark_pid_dir: /var/run/spark

# Spark logging directory
spark_log_dir: /var/log/spark

# Properties
hadoop_conf_dir: /etc/hadoop/conf
hadoop_home: "/opt/tdp/hadoop"
hdfs_user: hdfs

# Kerberos
###
# Set to 'no' to skip service principals and keytabs creation.
# This can be useful if TDP operators don't have admin access to the Kerberos server,
# or if the Kerberos server does not support MIT Kerberos tools.
###
krb_create_principals_keytabs: yes

# SSL Keystore and Truststore
spark_keystore_location: /etc/ssl/certs/keystore.jks
spark_keystore_password: Keystore123!
spark_truststore_location: /etc/ssl/certs/truststore.jks
spark_truststore_password: Truststore123!

# Spark History Server kerberos
spark_ui_spnego_principal: "HTTP/{{ ansible_fqdn }}@{{ realm }}"
spark_ui_spnego_keytab: /etc/security/keytabs/spnego.service.keytab

# spark-defaults.conf - common
spark_defaults_common:
  spark.acls.enable: "true"
  spark.ui.view.acls: spark
  spark.shuffle.service.port: "{{ spark_shuffle_service_port }}"
  ###
  # Controlling the Spark's ports for calibration with firewalls
  # If the spark.driver.port fails, it is incremented by 1 and retried.
  # This happens up to spark.port.maxRetries times.
  # spark.blockManager.port must be larger than spark.driver.port + sparkspark.sport.maxRetries
  # See tdp-cluster.yml for ports configuration
  ###
  spark.port.maxRetries: "{{ spark_port_max_retries }}"
  spark.driver.port: "{{ spark_driver_bind_port }}"
  spark.blockManager.port: "{{ spark_blockmanager_bind_port }}"
  spark.ui.port: "{{ spark_ui_bind_port }}"

# spark-default.conf - Spark Client
spark_defaults_client:
  spark.eventLog.dir: "hdfs://{{ cluster_name }}/spark-logs"
  spark.eventLog.enabled: "true"
  spark.hadoop.yarn.timeline-service.enabled: "false"
  spark.yarn.historyServer.address: "{{ groups['spark_hs'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ spark_hs_https_port }}"
  spark.master: yarn
  # Hive configuration
  spark.datasource.hive.warehouse.metastoreUri: |-
    {{ groups['hive_ms'] |
      map('tosit.tdp.access_fqdn', hostvars) |
      map('regex_replace', '^', 'thrift://')|
      map('regex_replace', '^(.*)$', '\1:' + hive_metastore_listener_port | string) |
      list |
      join(',') }}
  spark.hadoop.hive.zookeeper.quorum: |-
    {{ groups['zk'] |
      map('tosit.tdp.access_fqdn', hostvars) |
      map('regex_replace', '^(.*)$', '\1:' + zookeeper_server_client_port | string) |
      list |
      join(',') }}
  spark.sql.hive.metastore.jars: /opt/tdp/hive/lib/*
  # Default number of executors
  spark.executor.instances: 2
  # Default spark AM allocated vcores
  spark.yarn.am.cores: 1
  # Default spark AM allocated memory
  spark.yarn.am.memory: 640m
  # spark.yarn.am.memoryOverhead can be needed, default value: AM memory * 0.10, with minimum of 384
  # Default spark driver allocated vcores
  spark.driver.cores: 1
  # Default spark driver allocated memory
  spark.driver.memory: 640m
  # spark.driver.memoryOverhead can be needed, default value: driver memory * 0.10, with minimum of 384
  # Default spark executor allocated vcores
  spark.executor.cores: 1
  # Default spark executor allocated memory
  spark.executor.memory: 640m
  # spark.executor.memoryOverhead can be needed, default value: executor memory * 0.10, with minimum of 384

# spark-default.conf - Spark History Server
spark_defaults_hs:
  spark.history.kerberos.enabled: "true"
  spark.history.kerberos.keytab: /etc/security/keytabs/spark.service.keytab
  spark.history.kerberos.principal: "spark/{{ ansible_fqdn }}@{{ realm }}"
  spark.history.ui.acls.enable: "true"
  spark.history.ui.admin.acls: knox
  spark.history.ui.port: "{{ spark_hs_http_port }}"
  spark.ui.filters: org.apache.hadoop.security.authentication.server.AuthenticationFilter
  spark.org.apache.hadoop.security.authentication.server.AuthenticationFilter.params: type=kerberos,kerberos.principal={{ spark_ui_spnego_principal }},kerberos.keytab={{ spark_ui_spnego_keytab }},cookie.domain={{ http_cookie_domain }},signature.secret.file={{ http_secret_location }}
  spark.ssl.historyServer.enabled: "true"
  spark.ssl.historyServer.keyStore: "{{ spark_keystore_location }}"
  spark.ssl.historyServer.keyStorePassword: "{{ spark_keystore_password }}"
  spark.ssl.historyServer.port: "{{ spark_hs_https_port }}"
  spark.history.fs.logDirectory: "hdfs://{{ cluster_name }}/spark-logs"
  spark.history.provider: org.apache.spark.deploy.history.FsHistoryProvider
  spark.ui.proxyBase: "{% if 'knox' in groups and groups['knox'] %}/gateway/tdpldap/sparkhistory{% endif %}"

# spark-env.sh - common
spark_env_common:
  HADOOP_CONF_DIR: "{{ hadoop_conf_dir }}"
  HADOOP_HOME: "{{ hadoop_home }}"
  HIVE_CONF_DIR: "{{ spark_client_conf_dir }}"
  LD_LIBRARY_PATH: "'{{ hadoop_home }}/lib/native/:$LD_LIBRARY_PATH'"

# spark-env.sh - Spark History Server
spark_env_hs:
  SPARK_LOG_DIR: "{{ spark_log_dir }}"
  SPARK_DAEMON_JAVA_OPTS: "'{{ jmx_common_opts }} -Dcom.sun.management.jmxremote.password.file={{ spark_hs_conf_dir }}/jmxremote.password {{ jmx_exporter_hs_opts }}'"
  SPARK_DAEMON_MEMORY: "{{ spark2_hs_heapsize }}"

# spark-env.sh - Spark Client
spark_env_client:

# hive-site.xml - spark client
hive_site_spark:
  hive.metastore.uris: |-
    {{ groups['hive_ms'] |
      map('tosit.tdp.access_fqdn', hostvars) |
      map('regex_replace', '^', 'thrift://')|
      map('regex_replace', '^(.*)$', '\1:' + hive_metastore_listener_port | string) |
      list |
      join(',') }}
  metastore.thrift.uri.selection: RANDOM
  hive.exec.scratchdir: /tmp/spark
  hive.metastore.client.connect.retry.delay: 5
  hive.metastore.client.socket.timeout: 1800
  hive.server2.enable.doAs: "false"
  hive.server2.transport.mode: http
  hive.server2.thrift.port: "{{ hive_hiveserver2_thrift_port }}"
  hive.server2.thrift.http.port: "{{ hive_hiveserver2_thrift_http_port }}"
  hive.metastore.sasl.enabled: "true"
  hadoop.security.authentication: kerberos
  hive.metastore.kerberos.principal: "hive/_HOST@{{ realm }}"
  hive.metastore.execute.setugi: "true"
  hadoop.rpc.protection: AUTHENTICATION
  hive.execution.engine: spark

# Spark2 hs resources allocation
spark2_hs_heapsize: 1024m

# JMX exporter properties
jmx_exporter_hs_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_spark_hs_http_port }}:{{ jmx_exporter_conf_dir }}/{{ spark_version }}-hs.yml{% else %}{% endif %}"
jmxremote_username: jmxuser
jmxremote_password: Tdpjmx123,
jmx_exporter:
  startDelaySeconds: 0
  ssl: true
  username: "{{ jmxremote_username }}"
  password: "{{ jmxremote_password }}"
  lowercaseOutputName: false
  lowercaseOutputLabelNames: false

# Service start on boot policies
spark_hs_start_on_boot: no

# Service restart policies
spark_hs_restart: "no"
