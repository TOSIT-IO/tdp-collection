# Copyright 2022 TOSIT.IO
# SPDX-License-Identifier: Apache-2.0

---
# Hadoop version
hadoop_version: "3.4.2-1.0"
hadoop_release: "hadoop-{{ hadoop_version }}"
hadoop_dist_file: "{{ hadoop_release }}.tar.gz"

# Hadoop users and group
mapred_user: mapred
hadoop_group: hadoop

# Hadoop installation directory
hadoop_root_dir: "{{ tdp_root_dir }}"
hadoop_install_dir: "{{ hadoop_root_dir }}/hadoop"

# Hadoop configuration directories
hadoop_root_conf_dir: /etc/hadoop
hadoop_nn_conf_dir: "{{ hadoop_root_conf_dir }}/conf.nn"
hadoop_dn_conf_dir: "{{ hadoop_root_conf_dir }}/conf.dn"
hadoop_jn_conf_dir: "{{ hadoop_root_conf_dir }}/conf.jn"
hadoop_zkfc_conf_dir: "{{ hadoop_root_conf_dir }}/conf.zkfc"
hadoop_client_conf_dir: "{{ hadoop_root_conf_dir }}/conf"
hadoop_httpfs_conf_dir: "{{ hadoop_root_conf_dir }}/conf.httpfs"
hadoop_rm_conf_dir: "{{ hadoop_root_conf_dir }}/conf.rm"
hadoop_nm_conf_dir: "{{ hadoop_root_conf_dir }}/conf.nm"
hadoop_ats_conf_dir: "{{ hadoop_root_conf_dir }}/conf.ats"
hadoop_jhs_conf_dir: "{{ hadoop_root_conf_dir }}/conf.jhs"

# Hadoop pid directories
hadoop_pid_dir: /run/hadoop
hadoop_client_pid_dir: /run/hadoop/$USER


# Hadoop JVM options
jmx_common_opts: "-Dcom.sun.management.jmxremote=true"
jaas_nn_opts: "-Djava.security.auth.login.config={{ hadoop_nn_conf_dir }}/krb5JAASnn.conf"
jaas_rm_opts: "-Djava.security.auth.login.config={{ hadoop_rm_conf_dir }}/krb5JAASrm.conf"

# jmx exporter configuration files
hdfs_jmx_exporter_conf_file: "hdfs-jmx-exporter.yml"
yarn_jmx_exporter_conf_file: "yarn-jmx-exporter.yml"
mapred_jmx_exporter_conf_file: "mapred-jmx-exporter.yml"

# jmx-exporter
jmx_exporter_nn_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_hdfs_nn_http_port }}:{{ hadoop_root_conf_dir }}/{{ hdfs_jmx_exporter_conf_file }}{% endif %}"
jmx_exporter_zkfc_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_hdfs_zkfc_http_port }}:{{ hadoop_root_conf_dir }}/{{ hdfs_jmx_exporter_conf_file }}{% endif %}"
jmx_exporter_jn_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_hdfs_jn_http_port }}:{{ hadoop_root_conf_dir }}/{{ hdfs_jmx_exporter_conf_file }}{% endif %}"
jmx_exporter_dn_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_hdfs_dn_http_port }}:{{ hadoop_root_conf_dir }}/{{ hdfs_jmx_exporter_conf_file }}{% endif %}"
jmx_exporter_httpfs_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_hdfs_httpfs_http_port }}:{{ hadoop_root_conf_dir }}/{{ hdfs_jmx_exporter_conf_file }}{% endif %}"

jmx_exporter_rm_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_yarn_rm_http_port }}:{{ hadoop_root_conf_dir }}/{{ yarn_jmx_exporter_conf_file }}{% endif %}"
jmx_exporter_nm_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_yarn_nm_http_port }}:{{ hadoop_root_conf_dir }}/{{ yarn_jmx_exporter_conf_file }}{% endif %}"
jmx_exporter_ats_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_yarn_ats_http_port }}:{{ hadoop_root_conf_dir }}/{{ yarn_jmx_exporter_conf_file }}{% endif %}"
jmx_exporter_jhs_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_mapred_jhs_http_port }}:{{ hadoop_root_conf_dir }}/{{ mapred_jmx_exporter_conf_file }}{% endif %}"

# Hadoop logging configuration
# Root logger should be: [RFA | DRFA]
hadoop_root_logger: RFA
# Root logger level should be: [FATAL| ERROR| WARN| INFO| DEBUG| TRACE]
hadoop_root_logger_level: INFO
# Common appenders config
hadoop_log_file: hadoop.log
hadoop_env_client_log_dir: /var/log/hadoop/$USER
hadoop_log4j_client_log_dir: .
hadoop_log_layout_pattern: "{{ tdp_log_layout_pattern }}"
# DRFA appenders config
hadoop_log_drfa_date_pattern: "'.'yyyy-MM-dd"
# RFA appenders config
hadoop_log_rfa_maxfilesize: 256MB
hadoop_log_rfa_maxbackupindex: 10

# Kerberos
###
# Set to 'no' to skip service principals and keytabs creation.
# This can be useful if TDP operators don't have admin access to the Kerberos server,
# or if the Kerberos server does not support MIT Kerberos tools.
###
krb_create_principals_keytabs: true
zookeeper_headless_principal: zookeeper
hdfs_headless_principal: hdfs
yarn_headless_principal: yarn
mapred_headless_principal: mapred
hbase_headless_principal: hbase
hive_headless_principal: hive
spark_headless_principal: spark

# SSL Keystore and Truststore
hadoop_keystore_location: /etc/ssl/certs/keystore.jks
hadoop_keystore_password: Keystore123!
hadoop_truststore_location: /etc/ssl/certs/truststore.jks
hadoop_truststore_password: Truststore123!

ssl_server:
  ssl.server.keystore.location: "{{ hadoop_keystore_location }}"
  ssl.server.truststore.location: "{{ hadoop_truststore_location }}"

ssl_client:
  ssl.client.truststore.location: "{{ hadoop_truststore_location }}"

# Hadoop credentials
hadoop_credentials_store_file: hadoop.jceks
hadoop_credentials_properties:
  - property: ssl.server.keystore.password
    value: '{{ hadoop_keystore_password }}'
  - property: ssl.server.truststore.password
    value: '{{ hadoop_truststore_password }}'
  - property: ssl.client.truststore.password
    value: '{{ hadoop_truststore_password }}'

# SPNEGO Configuration
http_secret_location: /etc/security/http_secret
http_cookie_domain: tdp.local

# Properties
java_home: /usr/lib/jvm/jre-1.8.0-openjdk

hadoop_ha_zookeeper_quorum: |
  {{ groups['zk'] |
     map('tosit.tdp.access_fqdn', hostvars) |
     map('regex_replace', '^(.*)$', '\1:' + zookeeper_server_client_port | string) |
     list |
     join(',') }}

auth_to_local:
  hdfs:
    - RULE:[2:$1/$2@$0]([ndj]n/.*@{{ realm }})s/.*/hdfs/
    - RULE:[1:$1@$0]({{ hdfs_headless_principal }}@{{ realm }})s/.*/hdfs/
  yarn:
    - RULE:[2:$1/$2@$0]([rn]m/.*@{{ realm }})s/.*/yarn/
    - RULE:[1:$1@$0]({{ yarn_headless_principal }}@{{ realm }})s/.*/yarn/
  mapred:
    - RULE:[2:$1/$2@$0](jhs/.*@{{ realm }})s/.*/mapred/
    - RULE:[1:$1@$0]({{ mapred_headless_principal }}@{{ realm }})s/.*/mapred/
  hive:
    - RULE:[2:$1/$2@$0](hive/.*@{{ realm }})s/.*/hive/
    - RULE:[1:$1@$0]({{ hive_headless_principal }}@{{ realm }})s/.*/hive/
  hbase:
    - RULE:[1:$1@$0]({{ hbase_headless_principal }}@{{ realm }})s/.*/hbase/
  zookeeper:
    - RULE:[1:$1@$0]({{ zookeeper_headless_principal }}@{{ realm }})s/.*/zookeeper/
  spark:
    - RULE:[1:$1@$0]({{ spark_headless_principal }}@{{ realm }})s/.*/spark/
  ranger:
    - RULE:[2:$1/$2@$0](rangeradmin/.*@{{ realm }})s/.*/rangeradmin/
    - RULE:[2:$1/$2@$0](keyadmin/.*@{{ realm }})s/.*/keyadmin/
    - RULE:[2:$1/$2@$0](rangerusersync/.*@{{ realm }})s/.*/rangerusersync/
  httpfs:
    - RULE:[2:$1/$2@$0](httpfs/.*@{{ realm }})s/.*/hdfs/

# Rack awareness
rack_prefix: default

# core-site.xml - common
core_site:
  fs.defaultFS: "hdfs://{{ cluster_name }}"
  ha.zookeeper.quorum: "{{ hadoop_ha_zookeeper_quorum | trim }}"
  ha.zookeeper.acl: sasl:nn:rwcda
  hadoop.rpc.protection: authentication
  hadoop.security.authentication: kerberos
  hadoop.security.authorization: "true"
  hadoop.security.auth_to_local: |-
    {% for rule in (auth_to_local | dict2items | map(attribute='value') | flatten) %}
    {{ rule }}
    {% endfor %}
    DEFAULT
  hadoop.proxyuser.hbase.groups: "*"
  hadoop.proxyuser.hbase.hosts: "*"
  hadoop.proxyuser.hdfs.groups: "*"
  hadoop.proxyuser.hdfs.hosts: "*"
  hadoop.proxyuser.hive.groups: "*"
  hadoop.proxyuser.hive.hosts: "*"
  hadoop.proxyuser.knox.groups: "*"
  hadoop.proxyuser.knox.hosts: "*"
  hadoop.proxyuser.knox.users: "*"
  hadoop.proxyuser.phoenixqueryserver.hosts: "*"
  hadoop.proxyuser.phoenixqueryserver.groups: "*"
  # kerberos auth for the webuis
  hadoop.http.authentication.type: kerberos
  hadoop.http.authentication.kerberos.keytab: /etc/security/keytabs/spnego.service.keytab
  hadoop.http.authentication.kerberos.principal: "HTTP/_HOST@{{ realm }}"
  hadoop.http.filter.initializers: org.apache.hadoop.security.AuthenticationFilterInitializer
  hadoop.http.authentication.simple.anonymous.allowed: "false"
  hadoop.http.authentication.signature.secret.file: "{{ http_secret_location }}"
  hadoop.http.authentication.cookie.domain: "{{ http_cookie_domain }}"
  hadoop.ssl.server.conf: ssl-server.xml
  hadoop.ssl.client.conf: ssl-client.xml
  fs.trash.checkpoint.interval: 360
  hadoop.security.key.provider.path: "{% if 'ranger_kms' in groups and groups['ranger_kms'] %}{{ ranger_kms_url }}{% else %}{% endif %}"
  net.topology.script.file.name: "{{ hadoop_client_conf_dir }}/rack-topology.sh"

# mapred-site.xml
mapred_site:
  mapreduce.framework.name: yarn
  # Default allocated Memory for the mapper container
  mapreduce.map.memory.mb: 1024
  # Default allocated Memory for the reducer container
  mapreduce.reduce.memory.mb: 2048
  # Mapper Java Heap Size (included in mapreduce.map.memory.mb)
  mapreduce.map.java.opts: -Xmx768m
  # Reducer Java Heap Size (included in mapreduce.reduce.memory.mb)
  mapreduce.reduce.java.opts: -Xmx1536m
  # Default mapper allocated vcores
  mapreduce.map.cpu.vcores: 1
  # Default reducer allocated vcores
  mapreduce.reduce.cpu.vcores: 1
  # Default allocated memory for sort
  mapreduce.task.io.sort.mb: 100
  # Default mapreduce AM allocated memory
  yarn.app.mapreduce.am.resource.mb: 1536
  mapreduce.application.classpath: /opt/tdp/hadoop/share/hadoop/mapreduce/*,/opt/tdp/hadoop/share/hadoop/mapreduce/lib/*,/etc/hadoop/conf/
  # jobhistory conf
  mapreduce.jobhistory.address: "{{ groups['mapred_jhs'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ mapred_jhs_rpc_port }}"
  mapreduce.jobhistory.admin.address: "{{ groups['mapred_jhs'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ mapred_jhs_admin_port }}"
  mapreduce.jobhistory.webapp.address: "{{ groups['mapred_jhs'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ mapred_jhs_http_port }}"
  mapreduce.jobhistory.webapp.https.address: "{{ groups['mapred_jhs'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ mapred_jhs_https_port }}"
  mapreduce.jobhistory.intermediate-done-dir: /mr-history/tmp
  mapreduce.jobhistory.done-dir: /mr-history/done
  mapreduce.jobhistory.principal: jhs/_HOST@{{ realm }}
  mapreduce.jobhistory.keytab: /etc/security/keytabs/jhs.service.keytab
  mapreduce.jobhistory.bind-host: 0.0.0.0
  mapreduce.cluster.acls.enabled: "true"
  mapreduce.cluster.administrators: mapred,yarn,knox,hue
  mapreduce.jobhistory.admin.acl: "*"
  mapreduce.jobhistory.http.policy: HTTPS_ONLY
  mapreduce.jobhistory.webapp.spnego-principal: HTTP/_HOST@{{ realm }}
  mapreduce.jobhistory.webapp.spnego-keytab-file: /etc/security/keytabs/spnego.service.keytab
  ###
  # The yarn.app.mapreduce.am.job.client.port-range property controls the pool of ports available to MR job application masters.
  # After adding this to the mapred-site.xml, the following MR jobs will only use ports in this range
  # Default values are blank strings meaning unrestricted port range.
  ###
  yarn.app.mapreduce.am.job.client.port-range: "{{ mapred_am_bind_portrange }}"
  mapreduce.shuffle.port: "{{ mapred_sh_shuffle_port }}"

# Ranger KMS url
ranger_kms_url: "{% if 'ranger_kms' in groups and groups['ranger_kms'] %}kms://https@{{ ranger_kms_hosts }}:{{ ranger_kms_https_port }}/kms{% else %}NONE{% endif %}"
ranger_kms_hosts: |-
  {% if 'ranger_kms' in groups and groups['ranger_kms'] %}
  {{ groups['ranger_kms'] |
    map('tosit.tdp.access_fqdn', hostvars) |
    join(';') }}{% else %}NONE{% endif %}

hadoop_optional_tools:
  - hadoop-aws

# Custom opts
hadoop_client_custom_opts: ""
