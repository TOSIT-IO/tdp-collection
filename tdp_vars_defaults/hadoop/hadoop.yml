---
# Hadoop version
hadoop_release: hadoop-3.1.1-TDP-0.1.0-SNAPSHOT
hadoop_dist_file: "{{ hadoop_release }}.tar.gz"

# Hadoop users and group
mapred_user: mapred
hadoop_group: hadoop

# Hadoop installation directory
hadoop_root_dir: /opt/tdp
hadoop_install_dir: "{{ hadoop_root_dir }}/hadoop"

# Hadoop configuration directories
hadoop_root_conf_dir: /etc/hadoop
hadoop_nn_conf_dir: "{{ hadoop_root_conf_dir }}/conf.nn"
hadoop_dn_conf_dir: "{{ hadoop_root_conf_dir }}/conf.dn"
hadoop_jn_conf_dir: "{{ hadoop_root_conf_dir }}/conf.jn"
hadoop_zkfc_conf_dir: "{{ hadoop_root_conf_dir }}/conf.zkfc"
hadoop_client_conf_dir: "{{ hadoop_root_conf_dir }}/conf"
hadoop_rm_conf_dir: "{{ hadoop_root_conf_dir }}/conf.rm"
hadoop_nm_conf_dir: "{{ hadoop_root_conf_dir }}/conf.nm"
hadoop_ats_conf_dir: "{{ hadoop_root_conf_dir }}/conf.ats"
hadoop_jhs_conf_dir: "{{ hadoop_root_conf_dir }}/conf.jhs"

# Hadoop pid directories
hadoop_pid_dir: /run/hadoop
hadoop_client_pid_dir: /run/hadoop/$USER

# ZKFC options
hdfs_zkfc_opts: ""
hdfs_zkfc_nn_opts: "-Djava.security.auth.login.config={{ hadoop_nn_conf_dir }}/krb5JAASnn.conf"

# Hadoop logging directory
hadoop_log_dir: /var/log/hadoop
hadoop_env_client_log_dir: /var/log/hadoop/$USER
hadoop_log4j_client_log_dir: .

# SSL Keystore and Truststore
hadoop_keystore_location: /etc/ssl/certs/keystore.jks
hadoop_keystore_password: Keystore123!
hadoop_truststore_location: /etc/ssl/certs/truststore.jks
hadoop_truststore_password: Truststore123!

ssl_server:
  ssl.server.keystore.location: "{{ hadoop_keystore_location }}"
  ssl.server.keystore.password: "{{ hadoop_keystore_password }}"
  # ssl.server.keystore.keypassword: "{{ hadoop_keystore_password }}"
  ssl.server.truststore.location: "{{ hadoop_truststore_location }}"
  ssl.server.truststore.password: "{{ hadoop_truststore_password }}"

ssl_client:
  # ssl.client.keystore.location: "{{ hadoop_keystore_location }}"
  # ssl.client.keystore.password: "{{ hadoop_keystore_password }}"
  # ssl.client.keystore.keypassword: "{{ hadoop_keystore_password }}"
  ssl.client.truststore.location: "{{ hadoop_truststore_location }}"
  ssl.client.truststore.password: "{{ hadoop_truststore_password }}"

# Properties
java_home: /usr/lib/jvm/jre-1.8.0-openjdk

hadoop_ha_zookeeper_quorum: |
  {{ groups['zk'] | 
     map('tosit.tdp.access_fqdn', hostvars) |
     map('regex_replace', '^(.*)$', '\1:2181') |
     list |
     join(',') }}
     
# core-site.xml - common
core_site:
  fs.defaultFS: "hdfs://mycluster"
  ha.zookeeper.quorum: "{{ hadoop_ha_zookeeper_quorum | trim }}"
  ha.zookeeper.acl: sasl:nn:rwcda
  hadoop.rpc.protection: authentication
  hadoop.security.authentication: kerberos
  hadoop.security.authorization: "true"
  hadoop.security.auth_to_local: |
    RULE:[2:$1/$2@$0]([ndj]n/.*@{{ realm }})s/.*/hdfs/
    RULE:[2:$1/$2@$0]([rn]m/.*@{{ realm }})s/.*/yarn/
    RULE:[2:$1/$2@$0](jhs/.*@{{ realm }})s/.*/mapred/
    RULE:[2:$1/$2@$0](hive/.*@{{ realm }})s/.*/hive/
    DEFAULT
  hadoop.proxyuser.hbase.groups: "*"
  hadoop.proxyuser.hbase.hosts: "*"
  hadoop.proxyuser.hdfs.groups: "*"
  hadoop.proxyuser.hdfs.hosts: "*"
  hadoop.proxyuser.hive.groups: "*"
  hadoop.proxyuser.hive.hosts: "*"
  hadoop.proxyuser.knox.groups: "*"
  hadoop.proxyuser.knox.hosts: "*"
  hadoop.proxyuser.knox.users: "*"
  hadoop.proxyuser.oozie.hosts: "*"
  hadoop.proxyuser.oozie.groups: "*"
  hadoop.proxyuser.phoenixqueryserver.hosts: "*"
  hadoop.proxyuser.phoenixqueryserver.groups: "*"
  # kerberos auth for the webuis
  hadoop.http.authentication.type: kerberos
  hadoop.http.authentication.kerberos.keytab: /etc/security/keytabs/spnego.service.keytab
  hadoop.http.authentication.kerberos.principal: "HTTP/_HOST@{{ realm }}"
  hadoop.http.filter.initializers: org.apache.hadoop.security.AuthenticationFilterInitializer
  hadoop.ssl.server.conf: ssl-server.xml
  hadoop.ssl.client.conf: ssl-client.xml
  fs.trash.checkpoint.interval: 360

# mapred-site.xml
mapred_site:
  mapreduce.framework.name: yarn
  mapreduce.map.memory.mb: 1024
  mapreduce.reduce.memory.mb: 2048
  mapreduce.map.java.opts: -Xmx768m
  mapreduce.reduce.java.opts: -Xmx1536m
  mapreduce.application.classpath: /opt/tdp/hadoop/share/hadoop/mapreduce/*,/opt/tdp/hadoop/share/hadoop/mapreduce/lib/*,/etc/hadoop/conf/
<<<<<<< HEAD
  # jobhistory conf
  mapreduce.jobhistory.address: "{{ groups['mapred_jhs'][0] | tosit.tdp.access_fqdn(hostvars) }}:10201"
  mapreduce.jobhistory.webapp.address: "{{ groups['mapred_jhs'][0] | tosit.tdp.access_fqdn(hostvars) }}:19888"
  mapreduce.jobhistory.webapp.https.address: "{{ groups['mapred_jhs'][0] | tosit.tdp.access_fqdn(hostvars) }}:19890"
  mapreduce.jobhistory.intermediate-done-dir: /mr-history/tmp
  mapreduce.jobhistory.done-dir: /mr-history/done
  mapreduce.jobhistory.principal: jhs/_HOST@{{ realm }}
  mapreduce.jobhistory.keytab: /etc/security/keytabs/jhs.service.keytab
  mapreduce.jobhistory.bind-host: 0.0.0.0
  mapreduce.cluster.acls.enabled: true
  mapreduce.cluster.administrators: mapred,yarn,knox 
  mapreduce.jobhistory.admin.acl: "*" 
  mapreduce.jobhistory.http.policy: HTTPS_ONLY
  mapreduce.jobhistory.webapp.spnego-principal: HTTP/_HOST@{{ realm }} 
  mapreduce.jobhistory.webapp.spnego-keytab-file: /etc/security/keytabs/spnego.service.keytab
=======
  mapreduce.jobhistory.address: "{{ groups['yarn_ats'][0] | tosit.tdp.access_fqdn(hostvars) }}:10200"
  

# container-executor.cfg
container_executor:
  yarn.nodemanager.local-dirs: "{{ yarn_site['yarn.nodemanager.local-dirs'] }}"
  yarn.nodemanager.log-dirs: "{{ yarn_site['yarn.nodemanager.log-dirs'] }}"
  yarn.nodemanager.linux-container-executor.group: "{{ hadoop_group }}"
  banned.users: hdfs,yarn,mapred,bin
  min.user.id: 1000

# Ranger HDFS properties
ranger_hdfs_release: ranger-2.0.1-TDP-0.1.0-SNAPSHOT-hdfs-plugin
ranger_hdfs_dist_file: "{{ ranger_hdfs_release }}.tar.gz"
ranger_hdfs_install_dir: "{{ hadoop_root_dir }}/ranger-hdfs-plugin"
ranger_hdfs_install_properties:
  POLICY_MGR_URL: "https://{{ groups['ranger_admin'][0] | tosit.tdp.access_fqdn(hostvars) }}:6182"
  REPOSITORY_NAME: hdfs-tdp
  XAAUDIT_SOLR_ENABLE: "{% if 'ranger_solr' in groups and groups['ranger_solr'] %}true{% else %}false{% endif %}"
  XAAUDIT_SOLR_URL: "{% if 'ranger_solr' in groups and groups['ranger_solr'] %}http://{{ groups['ranger_solr'][0] | tosit.tdp.access_fqdn(hostvars) }}:8983/solr/ranger_audits{% else %}NONE{% endif %}"

# Ranger YARN properties
ranger_yarn_release: ranger-2.0.1-TDP-0.1.0-SNAPSHOT-yarn-plugin
ranger_yarn_dist_file: "{{ ranger_yarn_release }}.tar.gz"
ranger_yarn_install_dir: "{{ hadoop_root_dir }}/ranger-yarn-plugin"
ranger_yarn_install_properties:
  POLICY_MGR_URL: "https://{{ groups['ranger_admin'][0] | tosit.tdp.access_fqdn(hostvars) }}:6182"
  REPOSITORY_NAME: yarn-tdp
  XAAUDIT_SOLR_ENABLE: "{% if 'ranger_solr' in groups and groups['ranger_solr'] %}true{% else %}false{% endif %}"
  XAAUDIT_SOLR_URL: "{% if 'ranger_solr' in groups and groups['ranger_solr'] %}http://{{ groups['ranger_solr'][0] | tosit.tdp.access_fqdn(hostvars) }}:8983/solr/ranger_audits{% else %}NONE{% endif %}"

capacity_scheduler:
  yarn.scheduler.capacity.maximum-applications: 10000
  yarn.scheduler.capacity.maximum-am-resource-percent: 0.1
  yarn.scheduler.capacity.resource-calculator: org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
  yarn.scheduler.capacity.root.queues: default
  yarn.scheduler.capacity.root.default.capacity: 100
  yarn.scheduler.capacity.root.default.user-limit-factor: 1
  yarn.scheduler.capacity.root.default.maximum-capacity: 100
  yarn.scheduler.capacity.root.default.state: RUNNING
  yarn.scheduler.capacity.root.default.acl_submit_applications: "*"
  yarn.scheduler.capacity.root.default.acl_administer_queue: "*"
  yarn.scheduler.capacity.root.default.acl_application_max_priority: "*"
  yarn.scheduler.capacity.root.default.maximum-application-lifetime: -1
  yarn.scheduler.capacity.root.default.default-application-lifetime: -1
  yarn.scheduler.capacity.node-locality-delay: 40
  yarn.scheduler.capacity.rack-locality-additional-delay: -1
  yarn.scheduler.capacity.queue-mappings: ""
  yarn.scheduler.capacity.queue-mappings-override.enable: "false"
  yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments: 1
  yarn.scheduler.capacity.application.fail-fast: false

# Service restart policies
hdfs_nn_restart: "no"
hdfs_dn_restart: "no"
hdfs_jn_restart: "no"
hdfs_zkfc_restart: "no"
yarn_nm_restart: "no"
yarn_rm_restart: "no"
yarn_ts_restart: "no"
>>>>>>> eadfdc0 (ranger: enable solr audit logging if ranger_solr is in the inventory)
