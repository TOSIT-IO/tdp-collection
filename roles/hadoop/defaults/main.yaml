---
# Hadoop version
hadoop_release: hadoop-3.1.1-TDP-0.1.0-SNAPSHOT
hadoop_dist_file: "{{ hadoop_release }}.tar.gz"

# Hadoop users and group
hdfs_user: hdfs
yarn_user: yarn
mapred_user: mapred
hadoop_group: hadoop

# Hadoop installation directory
hadoop_root_dir: /opt/tdp
hadoop_install_dir: "{{ hadoop_root_dir }}/hadoop"

# Hadoop configuration directories
hadoop_root_conf_dir: /etc/hadoop
hadoop_nn_conf_dir: "{{ hadoop_root_conf_dir }}/conf.nn"
hadoop_dn_conf_dir: "{{ hadoop_root_conf_dir }}/conf.dn"
hadoop_jn_conf_dir: "{{ hadoop_root_conf_dir }}/conf.jn"
hadoop_zkfc_conf_dir: "{{ hadoop_root_conf_dir }}/conf.zkfc"
hadoop_client_conf_dir: "{{ hadoop_root_conf_dir }}/conf"
hadoop_rm_conf_dir: "{{ hadoop_root_conf_dir }}/conf.rm"
hadoop_nm_conf_dir: "{{ hadoop_root_conf_dir }}/conf.nm"
hadoop_ats_conf_dir: "{{ hadoop_root_conf_dir }}/conf.ats"

# Hadoop HDFS/YARN directories
hadoop_hdfs_dir: /var/lib/hdfs
hadoop_yarn_dir: /var/lib/yarn

# Hadoop pid directories
hadoop_pid_dir: /run/hadoop
hadoop_hdfs_pid_dir: /run/hadoop/hdfs
hadoop_yarn_pid_dir: /run/hadoop/yarn

# Hadoop logging directory
hadoop_log_dir: /var/log/hadoop
hadoop_hdfs_log_dir: /var/log/hadoop/hdfs
hadoop_yarn_log_dir: /var/log/hadoop/yarn

# SSL Keystore and Truststore
hadoop_keystore_location: /etc/ssl/certs/keystore.jks
hadoop_keystore_password: Keystore123!
hadoop_truststore_location: /etc/ssl/certs/truststore.jks
hadoop_truststore_password: Truststore123!

ssl_server:
  ssl.server.keystore.location: "{{ hadoop_keystore_location }}"
  ssl.server.keystore.password: "{{ hadoop_keystore_password }}"
  # ssl.server.keystore.keypassword: "{{ hadoop_keystore_password }}"
  ssl.server.truststore.location: "{{ hadoop_truststore_location }}"
  ssl.server.truststore.password: "{{ hadoop_truststore_password }}"

ssl_client:
  # ssl.client.keystore.location: "{{ hadoop_keystore_location }}"
  # ssl.client.keystore.password: "{{ hadoop_keystore_password }}"
  # ssl.client.keystore.keypassword: "{{ hadoop_keystore_password }}"
  ssl.client.truststore.location: "{{ hadoop_truststore_location }}"
  ssl.client.truststore.password: "{{ hadoop_truststore_password }}"

# Properties
java_home: /usr/lib/jvm/jre-1.8.0-openjdk

hadoop_ha_zookeeper_quorum: |
  {{ groups['zk'] | 
     map('tosit.tdp.access_fqdn', hostvars) |
     map('regex_replace', '^(.*)$', '\1:2181') |
     list |
     join(',') }}
     
# core-site.xml - common
core_site:
  fs.defaultFS: "hdfs://mycluster"
  ha.zookeeper.quorum: "{{ hadoop_ha_zookeeper_quorum | trim }}"
  hadoop.rpc.protection: authentication
  hadoop.security.authentication: kerberos
  hadoop.security.authorization: "true"
  hadoop.security.auth_to_local: |
    RULE:[2:$1/$2@$0]([ndj]n/.*@{{ realm }})s/.*/hdfs/
    RULE:[2:$1/$2@$0]([rn]m/.*@{{ realm }})s/.*/yarn/
    RULE:[2:$1/$2@$0](jhs/.*@{{ realm }})s/.*/mapred/
    RULE:[2:$1/$2@$0](hive/.*@{{ realm }})s/.*/hive/
    DEFAULT
  hadoop.proxyuser.hdfs.groups: "*"
  hadoop.proxyuser.hdfs.hosts: "*"
  hadoop.proxyuser.hive.groups: "*"
  hadoop.proxyuser.hive.hosts: "*"
  hadoop.proxyuser.knox.groups: "*"
  hadoop.proxyuser.knox.hosts: "*"
  hadoop.proxyuser.oozie.hosts: "*"
  hadoop.proxyuser.oozie.groups: "*"
  # kerberos auth for the webuis
  # hadoop.http.authentication.type: kerberos
  # hadoop.http.authentication.kerberos.keytab: /etc/security/keytabs/spnego.service.keytab
  # hadoop.http.authentication.kerberos.principal: "HTTP/_HOST@{{ realm }}"
  # hadoop.http.filter.initializers: org.apache.hadoop.security.AuthenticationFilterInitializer
  hadoop.ssl.server.conf: ssl-server.xml
  hadoop.ssl.client.conf: ssl-client.xml

# hdfs-site.xml - common
# TODO: make a hdfs_site per service: nn, jn, dn
hdfs_site:
  dfs.replication: 1
  dfs.nameservices: mycluster
  dfs.ha.namenodes.mycluster: nn1,nn2
  dfs.ha.fencing.methods: shell(/bin/true)
  dfs.ha.automatic-failover.enabled: "true"
  dfs.http.policy: HTTPS_ONLY
  dfs.data.transfer.protection: authentication
  dfs.web.authentication.kerberos.keytab: /etc/security/keytabs/spnego.service.keytab
  dfs.web.authentication.kerberos.principal: "HTTP/_HOST@{{ realm }}"
  # dfs.namenode.https-address: 0.0.0.0:9871
  dfs.namenode.rpc-address.mycluster.nn1: "{{ groups['hdfs_nn'][0] | tosit.tdp.access_fqdn(hostvars) }}:8020"
  dfs.namenode.rpc-address.mycluster.nn2: "{{ groups['hdfs_nn'][1] | tosit.tdp.access_fqdn(hostvars) }}:8020"
  dfs.namenode.http-address.mycluster.nn1: "{{ groups['hdfs_nn'][0] | tosit.tdp.access_fqdn(hostvars) }}:9870"
  dfs.namenode.http-address.mycluster.nn2: "{{ groups['hdfs_nn'][1] | tosit.tdp.access_fqdn(hostvars) }}:9870"
  dfs.namenode.https-address.mycluster.nn1: "{{ groups['hdfs_nn'][0] | tosit.tdp.access_fqdn(hostvars) }}:9871"
  dfs.namenode.https-address.mycluster.nn2: "{{ groups['hdfs_nn'][1] | tosit.tdp.access_fqdn(hostvars) }}:9871"
  dfs.namenode.name.dir: "{{ hadoop_hdfs_dir }}/nn"
  dfs.namenode.shared.edits.dir: |
    qjournal://{{ groups['hdfs_jn'] | 
       map('tosit.tdp.access_fqdn', hostvars) |
       map('regex_replace', '^(.*)$', '\1:8485') |
       list |
       join(';')
    }}/mycluster

  dfs.namenode.kerberos.principal: "nn/_HOST@{{ realm }}"
  dfs.namenode.keytab.file: /etc/security/keytabs/nn.service.keytab
  dfs.namenode.kerberos.internal.spnego.principal: "HTTP/_HOST@{{ realm }}"
  dfs.client.failover.proxy.provider.mycluster: org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
  dfs.journalnode.edits.dir: "{{ hadoop_hdfs_dir }}/jn"
  dfs.journalnode.rpc-address: 0.0.0.0:8485
  dfs.journalnode.https-address: 0.0.0.0:8481
  dfs.journalnode.kerberos.principal: jn/_HOST@{{ realm }}
  dfs.journalnode.keytab.file: /etc/security/keytabs/jn.service.keytab
  dfs.journalnode.kerberos.internal.spnego.principal: "HTTP/_HOST@{{ realm }}"
  dfs.block.access.token.enable: "true"
  dfs.datanode.address: 0.0.0.0:9866
  dfs.datanode.https.address: 0.0.0.0:9865
  dfs.datanode.data.dir: "{{ hadoop_hdfs_dir }}/dn"
  dfs.datanode.kerberos.principal: dn/_HOST@{{ realm }}
  dfs.datanode.keytab.file: /etc/security/keytabs/dn.service.keytab

# TODO: make a yarn_site per service: rm, nm, ts
yarn_site:
  hadoop.zk.address: "{{ hadoop_ha_zookeeper_quorum | trim }}"
  yarn.application.classpath: "$HADOOP_CONF_DIR, {{ hadoop_install_dir }}/share/hadoop/common/*, {{ hadoop_install_dir }}/share/hadoop/common/lib/*, {{ hadoop_install_dir }}/share/hadoop/hdfs/*, {{ hadoop_install_dir }}/share/hadoop/hdfs/lib/*, {{ hadoop_install_dir }}/share/hadoop/yarn/*, {{ hadoop_install_dir }}/share/hadoop/yarn/lib/*"
  yarn.http.policy: HTTPS_ONLY
  yarn.log-aggregation-enable: "true"
  yarn.nodemanager.remote-app-log-dir: "/app-logs"
  yarn.nodemanager.remote-app-log-dir-suffix : "logs"
  yarn.resourcemanager.ha.enabled: "true"
  yarn.resourcemanager.ha.rm-ids: "rm1,rm2"
  yarn.resourcemanager.cluster-id: "mycluster"
  yarn.resourcemanager.hostname.rm1: "{{ groups['yarn_rm'][0] | tosit.tdp.access_fqdn(hostvars) }}"
  yarn.resourcemanager.hostname.rm2: "{{ groups['yarn_rm'][1] | tosit.tdp.access_fqdn(hostvars) }}"
  yarn.resourcemanager.webapp.address.rm1: "{{ groups['yarn_rm'][0] | tosit.tdp.access_fqdn(hostvars) }}:8088"
  yarn.resourcemanager.webapp.address.rm2: "{{ groups['yarn_rm'][1] | tosit.tdp.access_fqdn(hostvars) }}:8088"
  yarn.resourcemanager.webapp.https.address.rm1: "{{ groups['yarn_rm'][0] | tosit.tdp.access_fqdn(hostvars) }}:8090"
  yarn.resourcemanager.webapp.https.address.rm2: "{{ groups['yarn_rm'][1] | tosit.tdp.access_fqdn(hostvars) }}:8090"
  yarn.resourcemanager.webapp.spnego-keytab-file: /etc/security/keytabs/spnego.service.keytab
  yarn.resourcemanager.webapp.spnego-principal: "HTTP/_HOST@{{ realm }}"
  yarn.resourcemanager.keytab: /etc/security/keytabs/rm.service.keytab
  yarn.resourcemanager.principal: "rm/_HOST@{{ realm }}"
  yarn.resourcemanager.system-metrics-publisher.enabled: "true"
  yarn.resourcemanager.recovery.enabled: "true"
  yarn.resourcemanager.store.class: "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"
  yarn.nodemanager.local-dirs: /data/yarn/local
  yarn.nodemanager.log-dirs: /data/yarn/logs
  yarn.nodemanager.container-executor.class: org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor
  yarn.nodemanager.linux-container-executor.group: hadoop
  yarn.nodemanager.aux-services: mapreduce_shuffle
  yarn.nodemanager.aux-services.mapreduce_shuffle.class: org.apache.hadoop.mapred.ShuffleHandler
  yarn.nodemanager.address: 0.0.0.0:45454
  yarn.nodemanager.bind-host: 0.0.0.0
  yarn.nodemanager.webapp.address: 0.0.0.0:8042
  yarn.nodemanager.webapp.https.address: 0.0.0.0:8044
  yarn.nodemanager.recovery.enabled: true
  yarn.nodemanager.recovery.dir: "{{ hadoop_yarn_dir }}"
  yarn.nodemanager.resource.cpu-vcores: 8
  yarn.nodemanager.resource.memory-mb: 8192
  yarn.scheduler.minimum-allocation-mb: 1024
  yarn.scheduler.maximum-allocation-mb: 8192
  yarn.nodemanager.principal: "nm/_HOST@{{ realm }}"
  yarn.nodemanager.keytab: /etc/security/keytabs/nm.service.keytab
  yarn.nodemanager.webapp.spnego-keytab-file: /etc/security/keytabs/spnego.service.keytab
  yarn.nodemanager.webapp.spnego-principal: "HTTP/_HOST@{{ realm }}"
  #yarn.nodemanager.vmem-check-enabled: "false"
  yarn.nodemanager.vmem-pmem-ratio: 4
  yarn.timeline-service.enabled: "true"
  yarn.timeline-service.generic-application-history.enabled: "true"
  yarn.timeline-service.hostname: "{{ groups['yarn_ats'][0] | tosit.tdp.access_fqdn(hostvars) }}"
  yarn.timeline-service.address: 0.0.0.0:10200
  yarn.timeline-service.webapp.https.address: "{{ groups['yarn_ats'][0] | tosit.tdp.access_fqdn(hostvars) }}:8190"
  yarn.timeline-service.principal: ats/_HOST@{{ realm }}
  yarn.timeline-service.keytab: /etc/security/keytabs/ats.service.keytab
  # To enable Kerberos on the ATS UI
  # yarn.timeline-service.http-authentication.type: kerberos
  # yarn.timeline-service.http-authentication.kerberos.principal: HTTP/_HOST@{{ realm }}
  # yarn.timeline-service.http-authentication.kerberos.keytab: /etc/security/keytabs/spnego.service.keytab
  yarn.acl.enable: "true"
  yarn.admin.acl: yarn


# mapred-site.xml
mapred_site:
  mapreduce.framework.name: yarn
  mapreduce.map.memory.mb: 1024
  mapreduce.reduce.memory.mb: 2048
  mapreduce.map.java.opts: -Xmx768m
  mapreduce.reduce.java.opts: -Xmx1536m
  mapreduce.application.classpath: /opt/tdp/hadoop/share/hadoop/mapreduce/*,/opt/tdp/hadoop/share/hadoop/mapreduce/lib/*,/etc/hadoop/conf/
  mapreduce.jobhistory.address: "{{ groups['yarn_ats'][0] | tosit.tdp.access_fqdn(hostvars) }}:10200"
  

# container-executor.cfg
container_executor:
  yarn.nodemanager.local-dirs: "{{ yarn_site['yarn.nodemanager.local-dirs'] }}"
  yarn.nodemanager.log-dirs: "{{ yarn_site['yarn.nodemanager.log-dirs'] }}"
  yarn.nodemanager.linux-container-executor.group: "{{ hadoop_group }}"
  banned.users: hdfs,yarn,mapred,bin
  min.user.id: 1000

# Ranger HDFS properties
ranger_hdfs_release: ranger-2.0.1-TDP-0.1.0-SNAPSHOT-hdfs-plugin
ranger_hdfs_dist_file: "{{ ranger_hdfs_release }}.tar.gz"
ranger_hdfs_install_dir: "{{ hadoop_root_dir }}/ranger-hdfs-plugin"
ranger_hdfs_install_properties:
  POLICY_MGR_URL: "https://{{ groups['ranger_admin'][0] | tosit.tdp.access_fqdn(hostvars) }}:6182"
  REPOSITORY_NAME: hdfs-tdp

# Ranger YARN properties
ranger_yarn_release: ranger-2.0.1-TDP-0.1.0-SNAPSHOT-yarn-plugin
ranger_yarn_dist_file: "{{ ranger_yarn_release }}.tar.gz"
ranger_yarn_install_dir: "{{ hadoop_root_dir }}/ranger-yarn-plugin"
ranger_yarn_install_properties:
  POLICY_MGR_URL: "https://{{ groups['ranger_admin'][0] | tosit.tdp.access_fqdn(hostvars) }}:6182"
  REPOSITORY_NAME: yarn-tdp

capacity_scheduler:
  yarn.scheduler.capacity.maximum-applications: 10000
  yarn.scheduler.capacity.maximum-am-resource-percent: 0.1
  yarn.scheduler.capacity.resource-calculator: org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
  yarn.scheduler.capacity.root.queues: default
  yarn.scheduler.capacity.root.default.capacity: 100
  yarn.scheduler.capacity.root.default.user-limit-factor: 1
  yarn.scheduler.capacity.root.default.maximum-capacity: 100
  yarn.scheduler.capacity.root.default.state: RUNNING
  yarn.scheduler.capacity.root.default.acl_submit_applications: "*"
  yarn.scheduler.capacity.root.default.acl_administer_queue: "*"
  yarn.scheduler.capacity.root.default.acl_application_max_priority: "*"
  yarn.scheduler.capacity.root.default.maximum-application-lifetime: -1
  yarn.scheduler.capacity.root.default.default-application-lifetime: -1
  yarn.scheduler.capacity.node-locality-delay: 40
  yarn.scheduler.capacity.rack-locality-additional-delay: -1
  yarn.scheduler.capacity.queue-mappings: ""
  yarn.scheduler.capacity.queue-mappings-override.enable: "false"
  yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments: 1
  yarn.scheduler.capacity.application.fail-fast: false

# Service restart policies
hdfs_nn_restart: "no"
hdfs_dn_restart: "no"
hdfs_jn_restart: "no"
hdfs_zkfc_restart: "no"
yarn_nm_restart: "no"
yarn_rm_restart: "no"
yarn_ts_restart: "no"
