# Copyright 2022 TOSIT.IO
# SPDX-License-Identifier: Apache-2.0

---
yarn_user: yarn

hadoop_yarn_dir: /var/lib/yarn
hadoop_yarn_pid_dir: /run/hadoop/yarn
hadoop_yarn_log_dir: /var/log/yarn
hadoop_yarn_resourcemanager_log_file: "yarn-resourcemanager_${HOSTNAME}.log"
hadoop_yarn_nodemanager_log_file: "yarn-nodemanager_${HOSTNAME}.log"
hadoop_yarn_timelineserver_log_file: "yarn-timelineserver_${HOSTNAME}.log"

hadoop_mapred_dir: /var/lib/mapred
hadoop_mapred_pid_dir: /run/hadoop/mapred
hadoop_mapred_log_dir: /var/log/mapred
hadoop_mapred_historyserver_log_file: "mapred-historyserver_${HOSTNAME}.log"


# Propertiess
hdfs_user: hdfs

# CGroups Properties
cgroups_enabled: true
cgroups_root_dir: /sys/fs/cgroup
cgroups_yarn_dirs:
 - path: "{{ cgroups_root_dir }}/cpu/yarn"
   mode: "0750"
   owner: "{{ yarn_user }}"
   group: "root"
 - path: "{{ cgroups_root_dir }}/memory/yarn"
   mode: "0750"
   owner: "{{ yarn_user }}"
   group: "root"
 - path: "{{ cgroups_root_dir }}/blkio/yarn"
   mode: "0750"
   owner: "{{ yarn_user }}"
   group: "root"
 - path: "{{ cgroups_root_dir }}/net_cls/yarn"
   mode: "0750"
   owner: "{{ yarn_user }}"
   group: "root"
 - path: "{{ cgroups_root_dir }}/devices/yarn"
   mode: "0750"
   owner: "{{ yarn_user }}"
   group: "root"

# TODO: make a yarn_site per service: rm, nm, ts
yarn_site:
  hadoop.zk.address: "{{ hadoop_ha_zookeeper_quorum | trim }}"
  hadoop.zk.acl: world:anyone:r,sasl:rm:rwcda
  yarn.application.classpath: "$HADOOP_CONF_DIR, {{ hadoop_install_dir }}/share/hadoop/common/*, {{ hadoop_install_dir }}/share/hadoop/common/lib/*, {{ hadoop_install_dir }}/share/hadoop/hdfs/*, {{ hadoop_install_dir }}/share/hadoop/hdfs/lib/*, {{ hadoop_install_dir }}/share/hadoop/yarn/*, {{ hadoop_install_dir }}/share/hadoop/yarn/lib/*"
  yarn.http.policy: HTTPS_ONLY
  yarn.log-aggregation-enable: "true"
  yarn.nodemanager.remote-app-log-dir: "/app-logs"
  yarn.nodemanager.remote-app-log-dir-suffix: "logs"
  yarn.resourcemanager.ha.enabled: "true"
  yarn.resourcemanager.ha.rm-ids: "rm1,rm2"
  yarn.resourcemanager.cluster-id: "{{ cluster_name }}"
  yarn.resourcemanager.address.rm1: "{{ groups['yarn_rm'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_jobs_port }}"
  yarn.resourcemanager.address.rm2: "{{ groups['yarn_rm'][1] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_jobs_port }}"
  yarn.resourcemanager.scheduler.address.rm1: "{{ groups['yarn_rm'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_scheduler_port }}"
  yarn.resourcemanager.scheduler.address.rm2: "{{ groups['yarn_rm'][1] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_scheduler_port }}"
  yarn.resourcemanager.resource-tracker.address.rm1: "{{ groups['yarn_rm'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_tracker_port }}"
  yarn.resourcemanager.resource-tracker.address.rm2: "{{ groups['yarn_rm'][1] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_tracker_port }}"
  yarn.resourcemanager.admin.address.rm1: "{{ groups['yarn_rm'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_admin_port }}"
  yarn.resourcemanager.admin.address.rm2: "{{ groups['yarn_rm'][1] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_admin_port }}"
  yarn.resourcemanager.webapp.address.rm1: "{{ groups['yarn_rm'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_http_port }}"
  yarn.resourcemanager.webapp.address.rm2: "{{ groups['yarn_rm'][1] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_http_port }}"
  yarn.resourcemanager.webapp.https.address.rm1: "{{ groups['yarn_rm'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_https_port }}"
  yarn.resourcemanager.webapp.https.address.rm2: "{{ groups['yarn_rm'][1] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_rm_https_port }}"
  yarn.resourcemanager.webapp.spnego-keytab-file: /etc/security/keytabs/spnego.service.keytab
  yarn.resourcemanager.webapp.spnego-principal: "HTTP/_HOST@{{ realm }}"
  yarn.resourcemanager.keytab: /etc/security/keytabs/rm.service.keytab
  yarn.resourcemanager.principal: "rm/_HOST@{{ realm }}"
  yarn.resourcemanager.system-metrics-publisher.enabled: "true"
  yarn.resourcemanager.recovery.enabled: "true"
  yarn.resourcemanager.store.class: "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"
  yarn.nodemanager.local-dirs: "{{ yarn_nodemanager_data_dirs }}"
  yarn.nodemanager.log-dirs: "{{ yarn_nodemanager_applogs_dirs }}"
  yarn.resourcemanager.zk-state-store.parent-path: "/rmstore"
  yarn.resourcemanager.ha.automatic-failover.zk-base-path: "/yarn-leader-election"
  yarn.nodemanager.container-executor.class: org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor
  yarn.nodemanager.linux-container-executor.group: hadoop
  yarn.nodemanager.aux-services: mapreduce_shuffle
  yarn.nodemanager.aux-services.mapreduce_shuffle.class: org.apache.hadoop.mapred.ShuffleHandler
  yarn.nodemanager.address: "0.0.0.0:{{ yarn_nm_rpc_port }}"
  yarn.nodemanager.bind-host: 0.0.0.0
  yarn.nodemanager.localizer.address: "0.0.0.0:{{ yarn_nm_localizer_port }}"
  yarn.nodemanager.webapp.address: "0.0.0.0:{{ yarn_nm_http_port }}"
  yarn.nodemanager.webapp.https.address: 0.0.0.0:{{ yarn_nm_https_port }}
  # Resource Allocation
  # allocated dirs for nodemanager recovery if enabled
  yarn.nodemanager.recovery.enabled: true
  yarn.nodemanager.recovery.dir: "{{ hadoop_yarn_dir }}"
  # YARN allocated cpu pourcentage per worker
  yarn.nodemanager.resource.percentage-physical-cpu-limit: 80
  # YARN number of vcores per worker (depends on worker cpu cores * yarn.nodemanager.resource.percentage-physical-cpu-limit)
  # If CGroups are enabled, it must be the same as physical cpu cores * yarn.nodemanager.resource.percentage-physical-cpu-limit
  yarn.nodemanager.resource.cpu-vcores: 8
  # YARN allocated memory per worker
  yarn.nodemanager.resource.memory-mb: 8192
  # Minimum allocated memory per container
  yarn.scheduler.minimum-allocation-mb: 1024
  # Maximum allocated memory per container
  yarn.scheduler.maximum-allocation-mb: 8192
  # Minimum allocated vcores per container
  yarn.scheduler.minimum-allocation-vcores: 1
  # Maximum allocated vcores per container
  yarn.scheduler.maximum-allocation-vcores: 4
  yarn.nodemanager.principal: "nm/_HOST@{{ realm }}"
  yarn.nodemanager.keytab: /etc/security/keytabs/nm.service.keytab
  yarn.nodemanager.webapp.spnego-keytab-file: /etc/security/keytabs/spnego.service.keytab
  yarn.nodemanager.webapp.spnego-principal: "HTTP/_HOST@{{ realm }}"
  # yarn.nodemanager.vmem-check-enabled: "false"
  yarn.nodemanager.vmem-pmem-ratio: 4
  yarn.timeline-service.enabled: "true"
  yarn.timeline-service.generic-application-history.enabled: "true"
  yarn.timeline-service.client.best-effort: true
  yarn.timeline-service.client.max-retries: 3
  yarn.timeline-service.hostname: "{{ groups['yarn_ats'][0] | tosit.tdp.access_fqdn(hostvars) }}"
  yarn.timeline-service.address: "0.0.0.0:{{ yarn_ats_rpc_port }}"
  yarn.timeline-service.webapp.https.address: "{{ groups['yarn_ats'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ yarn_ats_https_port }}"
  yarn.timeline-service.principal: ats/_HOST@{{ realm }}
  yarn.timeline-service.keytab: /etc/security/keytabs/ats.service.keytab
  # To enable Kerberos on the ATS UI
  yarn.timeline-service.http-authentication.type: kerberos
  yarn.timeline-service.http-authentication.kerberos.principal: HTTP/_HOST@{{ realm }}
  yarn.timeline-service.http-authentication.kerberos.keytab: /etc/security/keytabs/spnego.service.keytab
  yarn.acl.enable: "true"
  yarn.admin.acl: yarn,knox
  yarn.log.server.url: "https://{{ groups['mapred_jhs'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ mapred_jhs_https_port }}/jobhistory/logs"
  # To enable cgroups
  yarn.nodemanager.linux-container-executor.resources-handler.class: "{% if cgroups_enabled %}org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler{% else %}org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler{% endif %}"
  yarn.nodemanager.linux-container-executor.cgroups.mount: "false"
  yarn.nodemanager.linux-container-executor.cgroups.mount-path: "{{ cgroups_root_dir }}"
  yarn.nodemanager.linux-container-executor.cgroups.hierarchy: "/yarn"
  yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage: "{% if cgroups_enabled %}true{% else %}false{% endif %}"


# container-executor.cfg
container_executor:
  yarn.nodemanager.local-dirs: "{{ yarn_site['yarn.nodemanager.local-dirs'] }}"
  yarn.nodemanager.log-dirs: "{{ yarn_site['yarn.nodemanager.log-dirs'] }}"
  yarn.nodemanager.linux-container-executor.group: "{{ hadoop_group }}"
  banned.users: hdfs,yarn,mapred,bin
  min.user.id: 1000
  # To enable cgroups
  root: "{{ cgroups_root_dir }}"
  yarn-hierarchy: "yarn"


# Ranger YARN properties
ranger_yarn_release: ranger-2.0.1-TDP-0.1.0-SNAPSHOT-yarn-plugin
ranger_yarn_dist_file: "{{ ranger_yarn_release }}.tar.gz"
ranger_yarn_install_dir: "{{ hadoop_root_dir }}/ranger-yarn-plugin"
ranger_yarn_install_properties:
  POLICY_MGR_URL: "https://{{ groups['ranger_admin'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ ranger_adm_https_port }}"
  REPOSITORY_NAME: yarn-tdp
  XAAUDIT_SOLR_ENABLE: "{% if 'ranger_solr' in groups and groups['ranger_solr'] %}true{% else %}false{% endif %}"
  XAAUDIT_SOLR_URL: "{% if 'ranger_solr' in groups and groups['ranger_solr'] %}http://{{ groups['ranger_solr'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ ranger_solr_http_port }}/solr/ranger_audits{% else %}NONE{% endif %}"

capacity_scheduler:
  yarn.scheduler.capacity.maximum-applications: 10000
  yarn.scheduler.capacity.maximum-am-resource-percent: 0.1
  yarn.scheduler.capacity.resource-calculator: org.apache.hadoop.yarn.util.resource.DominantResourceCalculator
  yarn.scheduler.capacity.root.queues: default
  yarn.scheduler.capacity.root.default.capacity: 100
  yarn.scheduler.capacity.root.default.user-limit-factor: 1
  yarn.scheduler.capacity.root.default.maximum-capacity: 100
  yarn.scheduler.capacity.root.default.state: RUNNING
  yarn.scheduler.capacity.root.default.acl_submit_applications: "*"
  yarn.scheduler.capacity.root.default.acl_administer_queue: "*"
  yarn.scheduler.capacity.root.default.acl_application_max_priority: "*"
  yarn.scheduler.capacity.root.default.maximum-application-lifetime: -1
  yarn.scheduler.capacity.root.default.default-application-lifetime: -1
  yarn.scheduler.capacity.node-locality-delay: 40
  yarn.scheduler.capacity.rack-locality-additional-delay: -1
  yarn.scheduler.capacity.queue-mappings: ""
  yarn.scheduler.capacity.queue-mappings-override.enable: "false"
  yarn.scheduler.capacity.per-node-heartbeat.maximum-offswitch-assignments: 1
  yarn.scheduler.capacity.application.fail-fast: false

# YARN resources allocation
yarn_resourcemanager_heapsize: 1024m
yarn_nodemanager_heapsize: 1024m
yarn_timelineserver_heapsize: 1024m
yarn_jobhistoryserver_heapsize: 1024m

yarn_nodemanager_data_dirs: "/data/yarn/local"
yarn_nodemanager_applogs_dirs: "/data/yarn/logs"

# Service start on boot policies
yarn_nm_start_on_boot: false
yarn_rm_start_on_boot: false
yarn_ts_start_on_boot: false
mapred_jhs_start_on_boot: false

# Service restart policies
yarn_nm_restart: "no"
yarn_rm_restart: "no"
yarn_ts_restart: "no"
mapred_jhs_restart: "no"

# YARN NodeManagers decommission
# List of NodeManagers to decommission. See https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.4/administration/content/decommissioning-slave-nodes.html
yarn_nodemanagers_decommission: []
