# Copyright 2022 TOSIT.IO
# SPDX-License-Identifier: Apache-2.0

---
# Spark version
spark_version: spark3
spark_release: spark-3.2.2-TDP-0.1.0-SNAPSHOT-bin-tdp
spark_dist_file: "{{ spark_release }}.tgz"

# Spark HBase Connector
spark_hbase_connector_enable: false
hbase_install_dir: "{{ spark_root_dir }}/hbase"

# Spark users and group
spark_user: spark
hadoop_group: hadoop

# Spark installation directory
spark_root_dir: /opt/tdp
spark_install_dir: "{{ spark_root_dir }}/spark3"

# Spark configuration directories
spark_conf_dir: /etc/spark3
spark_client_conf_dir: "{{ spark_conf_dir }}/conf"
spark_hs_conf_dir: "{{ spark_conf_dir }}/conf.hs"

# Spark pid directories
spark_pid_dir: /var/run/spark3

#Spark3 logging configuration
# Root logger should be: [RFA | DRFA]
spark_root_logger: RFA
# Root logger level should be: [FATAL| ERROR| WARN| INFO| DEBUG| TRACE]
spark_root_logger_level: INFO
# Common appenders config
spark_log_dir: "{{ spark3_log_dir }}"
spark_hs_log_file: "{{ spark3_hs_log_file }}"
spark_log_layout_pattern: "%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n"
# DRFA appenders config
spark_log_drfa_date_pattern: "'.'yyyy-MM-dd-HH-mm"
# RFA appenders config
spark_log_rfa_maxfilesize: 256MB
spark_log_rfa_maxbackupindex: 10

# Properties
hadoop_conf_dir: /etc/hadoop/conf
hadoop_home: "/opt/tdp/hadoop"
hdfs_user: hdfs

# Kerberos
###
# Set to 'no' to skip service principals and keytabs creation.
# This can be useful if TDP operators don't have admin access to the Kerberos server,
# or if the Kerberos server does not support MIT Kerberos tools.
###
krb_create_principals_keytabs: yes

# SSL Keystore and Truststore
spark_keystore_location: /etc/ssl/certs/keystore.jks
spark_keystore_password: Keystore123!
spark_truststore_location: /etc/ssl/certs/truststore.jks
spark_truststore_password: Truststore123!

# Hadoop credentials
hadoop_credentials_store_file: spark.jceks
hadoop_credentials_properties:
  - property: spark.ssl.historyServer.keyStorePassword
    value: '{{ spark_keystore_password }}'

# Spark History Server kerberos
spark_ui_spnego_principal: "HTTP/{{ ansible_fqdn }}@{{ realm }}"
spark_ui_spnego_keytab: /etc/security/keytabs/spnego.service.keytab

# spark-defaults.conf - common
spark_defaults_common:
  spark.acls.enable: "true"
  spark.ui.view.acls: spark
  spark.shuffle.service.port: "{{ spark3_shuffle_service_port }}"
  ###
  # Controlling the Spark's ports for calibration with firewalls
  # If the spark.driver.port fails, it is incremented by 1 and retried.
  # This happens up to spark.port.maxRetries times.
  # spark.blockManager.port must be larger than spark.driver.port + sparkspark.sport.maxRetries
  # See tdp-cluster.yml for ports configuration
  ###
  spark.port.maxRetries: "{{ spark3_port_max_retries }}"
  spark.driver.port: "{{ spark3_driver_bind_port }}"
  spark.blockManager.port: "{{ spark3_blockmanager_bind_port }}"
  spark.ui.port: "{{ spark3_ui_bind_port }}"

# spark-defaults.conf - Spark Client
spark_defaults_client:
  spark.eventLog.dir: "hdfs://{{ cluster_name }}/spark3-logs"
  spark.eventLog.enabled: "true"
  spark.hadoop.yarn.timeline-service.enabled: "false"
  spark.yarn.historyServer.address: "{{ groups['spark3_hs'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ spark3_hs_https_port }}"
  spark.yarn.appMasterEnv.PYSPARK_PYTHON: python3
  spark.master: yarn
  # Hive configuration
  spark.datasource.hive.warehouse.metastoreUri: |-
    {{ groups['hive_ms'] |
      map('tosit.tdp.access_fqdn', hostvars) |
      map('regex_replace', '^', 'thrift://')|
      map('regex_replace', '^(.*)$', '\1:' + hive_metastore_listener_port | string) |
      list |
      join(',') }}
  spark.hadoop.hive.zookeeper.quorum: |-
    {{ groups['zk'] |
      map('tosit.tdp.access_fqdn', hostvars) |
      map('regex_replace', '^(.*)$', '\1:' + zookeeper_server_client_port | string) |
      list |
      join(',') }}
  spark.sql.hive.metastore.jars: /opt/tdp/hive/lib/*
  # Default number of executors
  spark.executor.instances: 2
  # Default spark AM allocated vcores
  spark.yarn.am.cores: 1
  # Default spark AM allocated memory
  spark.yarn.am.memory: 640m
  # spark.yarn.am.memoryOverhead can be needed, default value: AM memory * 0.10, with minimum of 384
  # Default spark driver allocated vcores
  spark.driver.cores: 1
  # Default spark driver allocated memory
  spark.driver.memory: 640m
  # spark.driver.memoryOverhead can be needed, default value: driver memory * 0.10, with minimum of 384
  # Default spark executor allocated vcores
  spark.executor.cores: 1
  # Default spark executor allocated memory
  spark.executor.memory: 640m
  # spark.executor.memoryOverhead can be needed, default value: executor memory * 0.10, with minimum of 384

# spark-defaults.conf - Spark History Server
spark_defaults_hs:
  spark.history.kerberos.enabled: "true"
  spark.history.kerberos.keytab: /etc/security/keytabs/spark.service.keytab
  spark.history.kerberos.principal: "spark/{{ ansible_fqdn }}@{{ realm }}"
  spark.history.ui.acls.enable: "true"
  spark.history.ui.admin.acls: "knox"
  spark.history.ui.port: "{{ spark3_hs_http_port }}"
  spark.ui.filters: org.apache.hadoop.security.authentication.server.AuthenticationFilter
  spark.org.apache.hadoop.security.authentication.server.AuthenticationFilter.params: type=kerberos,kerberos.principal={{ spark_ui_spnego_principal }},kerberos.keytab={{ spark_ui_spnego_keytab }},cookie.domain={{ http_cookie_domain }},signature.secret.file={{ http_secret_location }}
  spark.ssl.historyServer.enabled: "true"
  spark.ssl.historyServer.keyStore: "{{ spark_keystore_location }}"
  spark.ssl.historyServer.port: "{{ spark3_hs_https_port }}"
  spark.history.fs.logDirectory: "hdfs://{{ cluster_name }}/spark3-logs"
  spark.history.provider: org.apache.spark.deploy.history.FsHistoryProvider
  spark.ui.proxyBase: "{% if 'knox' in groups and groups['knox'] %}/gateway/tdpldap/spark3history{% endif %}"
  spark.hadoop.hadoop.security.credential.provider.path: localjceks://file{{ spark_hs_conf_dir }}/{{ hadoop_credentials_store_file }}

# spark-env.sh - common
spark_env_common:
  PYSPARK_PYTHON: python3
  HADOOP_CONF_DIR: "{{ hadoop_conf_dir }}"
  HADOOP_HOME: "{{ hadoop_home }}"
  HIVE_CONF_DIR: "{{ spark_client_conf_dir }}"
  LD_LIBRARY_PATH: "'{{ hadoop_home }}/lib/native/:$LD_LIBRARY_PATH'"

# spark-env.sh - Spark History Server
spark_env_hs:
  SPARK_LOG_DIR: "{{ spark_log_dir }}"
  SPARK_DAEMON_JAVA_OPTS: "'{{ jmx_common_opts }} -Dcom.sun.management.jmxremote.password.file={{ spark_hs_conf_dir }}/jmxremote.password {{ jmx_exporter_hs_opts }}'"
  SPARK_DAEMON_MEMORY: "{{ spark3_hs_heapsize }}"

# spark-env.sh - Spark Client
spark_env_client:

# hive-site.xml - spark client
hive_site_spark:
  hive.metastore.uris: |-
    {{ groups['hive_ms'] |
      map('tosit.tdp.access_fqdn', hostvars) |
      map('regex_replace', '^', 'thrift://')|
      map('regex_replace', '^(.*)$', '\1:' + hive_metastore_listener_port | string) |
      list |
      join(',') }}
  metastore.thrift.uri.selection: RANDOM
  hive.exec.scratchdir: /tmp/spark
  hive.metastore.client.connect.retry.delay: 5
  hive.metastore.client.socket.timeout: 1800
  hive.server2.enable.doAs: "false"
  hive.server2.transport.mode: http
  hive.server2.thrift.port: "{{ hive_hiveserver2_thrift_port }}"
  hive.server2.thrift.http.port: "{{ hive_hiveserver2_thrift_http_port }}"
  hive.metastore.sasl.enabled: "true"
  hadoop.security.authentication: kerberos
  hive.metastore.kerberos.principal: "hive/_HOST@{{ realm }}"
  hive.metastore.execute.setugi: "true"
  hadoop.rpc.protection: AUTHENTICATION
  hive.execution.engine: spark

# Spark3 hs resources allocation
spark3_hs_heapsize: 1024m

# JMX exporter properties
jmx_exporter_hs_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_spark3_hs_http_port }}:{{ jmx_exporter_conf_dir }}/{{ spark_version }}-hs.yml{% else %}{% endif %}"
jmxremote_username: jmxuser
jmxremote_password: Tdpjmx123,
jmx_exporter:
  startDelaySeconds: 0
  ssl: true
  username: "{{ jmxremote_username }}"
  password: "{{ jmxremote_password }}"
  lowercaseOutputName: false
  lowercaseOutputLabelNames: false

# Service start on boot policies
spark_hs_start_on_boot: no

# Service restart policies
spark_hs_restart: "no"
