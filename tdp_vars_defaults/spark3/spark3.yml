# Copyright 2022 TOSIT.IO
# SPDX-License-Identifier: Apache-2.0

---
# Spark version
spark_version: spark3
spark_release_version: spark-3.5.8-1.0
spark_release: "{{ spark_release_version }}-bin-tdp"
spark_dist_file: "{{ spark_release }}.tgz"

# Spark HBase Connector
spark_hbase_connector_enable: false
hbase_install_dir: "{{ spark_root_dir }}/hbase"

# Iceberg Spark runtime
spark_iceberg_enable: true
iceberg_version: 1.4.3-0.0
iceberg_spark_runtime_release: "iceberg-spark-runtime-3.5_2.12-{{ iceberg_version }}"
iceberg_spark_runtime_dist_file: "{{ iceberg_spark_runtime_release }}.jar"
iceberg_mr_dist_release: "iceberg-mr-{{ iceberg_version }}"
iceberg_mr_dist_file: "{{ iceberg_mr_dist_release }}.jar"

# Spark users and group
spark_user: spark
hadoop_group: hadoop

# Spark installation directory
spark_root_dir: "{{ tdp_root_dir }}"
spark_install_dir: "{{ spark_root_dir }}/spark3"
spark_yarn_shuffle_path: "{{ spark_install_dir }}/yarn/{{ spark_release_version }}-yarn-shuffle.jar"

# Spark configuration directories
spark_conf_dir: /etc/spark3
spark_client_conf_dir: "{{ spark_conf_dir }}/conf"
spark_hs_conf_dir: "{{ spark_conf_dir }}/conf.hs"

# Spark pid directories
spark_pid_dir: /var/run/spark3

# SparkR
spark_enable_r: false

#Spark3 logging configuration
# Root logger should be: [RFA | DRFA]
spark_root_logger: RFA
# Root logger level should be: [FATAL| ERROR| WARN| INFO| DEBUG| TRACE]
spark_root_logger_level: INFO
# Common appenders config
spark_log_dir: "{{ spark3_log_dir }}"
spark_hs_log_file: "{{ spark3_hs_log_file }}"
spark_log_layout_pattern: "{{ tdp_log_layout_pattern }}"
# DRFA appenders config
spark_log_drfa_date_pattern: "%d{yyyy-MM-dd-HH}"
# RFA appenders config
spark_log_rfa_maxfilesize: 256MB
spark_log_rfa_maxbackupindex: 10

# Properties
hadoop_conf_dir: /etc/hadoop/conf
hadoop_home: "/opt/tdp/hadoop"
hdfs_user: hdfs

# Kerberos
###
# Set to 'no' to skip service principals and keytabs creation.
# This can be useful if TDP operators don't have admin access to the Kerberos server,
# or if the Kerberos server does not support MIT Kerberos tools.
###
krb_create_principals_keytabs: yes

# SSL Keystore and Truststore
spark_keystore_location: /etc/ssl/certs/keystore.jks
spark_keystore_password: Keystore123!
spark_truststore_location: /etc/ssl/certs/truststore.jks
spark_truststore_password: Truststore123!

# Hadoop credentials
hadoop_credentials_store_file: spark.jceks
hadoop_credentials_properties:
  - property: spark.ssl.historyServer.keyStorePassword
    value: '{{ spark_keystore_password }}'

# Spark History Server kerberos
spark_ui_spnego_principal: "HTTP/{{ ansible_fqdn }}@{{ realm }}"
spark_ui_spnego_keytab: /etc/security/keytabs/spnego.service.keytab

# spark-defaults.conf - common
spark_defaults_common:
  spark.acls.enable: "true"
  spark.ui.view.acls: spark
  spark.shuffle.service.port: "{{ spark3_shuffle_service_port }}"
  ###
  # Controlling the Spark's ports for calibration with firewalls
  # If the spark.driver.port fails, it is incremented by 1 and retried.
  # This happens up to spark.port.maxRetries times.
  # spark.blockManager.port must be larger than spark.driver.port + sparkspark.sport.maxRetries
  # See tdp-cluster.yml for ports configuration
  ###
  spark.port.maxRetries: "{{ spark3_port_max_retries }}"
  spark.driver.port: "{{ spark3_driver_bind_port }}"
  spark.blockManager.port: "{{ spark3_blockmanager_bind_port }}"
  spark.ui.port: "{{ spark3_ui_bind_port }}"
  spark.dynamicAllocation.enabled: "{{ spark_dynamic_allocation }}"
  spark.shuffle.service.enabled: "{{ spark_dynamic_allocation }}"
  spark.dynamicAllocation.shuffleTracking.enabled: "{{ spark_dynamic_allocation }}"
  spark.dynamicAllocation.minExecutors: 0
  spark.dynamicAllocation.initialExecutors: 1
  spark.dynamicAllocation.maxExecutors: "{{ groups['yarn_nm'] | default([]) | length | default(3, true) }}"


# spark-defaults.conf - Spark Client
spark_defaults_client:
  spark.eventLog.dir: "hdfs://{{ cluster_name }}/spark3-logs"
  spark.sql.warehouse.dir: "hdfs://{{ cluster_name }}/{{ spark_version }}-data"
  spark.eventLog.enabled: "true"
  spark.hadoop.yarn.timeline-service.enabled: "false"
  spark.yarn.historyServer.address: "{{ groups['spark3_hs'][0] | tosit.tdp.access_fqdn(hostvars) }}:{{ spark3_hs_https_port }}"
  spark.yarn.appMasterEnv.PYSPARK_PYTHON: python3
  spark.master: yarn
  # Hive configuration
  spark.datasource.hive.warehouse.metastoreUri: |-
    {{ groups['hive_ms'] |
      map('tosit.tdp.access_fqdn', hostvars) |
      map('regex_replace', '^', 'thrift://')|
      map('regex_replace', '^(.*)$', '\1:' + hive_metastore_listener_port | string) |
      list |
      join(',') }}
  spark.hadoop.hive.zookeeper.quorum: |-
    {{ groups['zk'] |
      map('tosit.tdp.access_fqdn', hostvars) |
      map('regex_replace', '^(.*)$', '\1:' + zookeeper_server_client_port | string) |
      list |
      join(',') }}
  spark.sql.hive.metastore.jars.path: /opt/tdp/hive/lib/
  spark.submit.deployMode: cluster
  # Default number of executors
  spark.executor.instances: 2
  # Default spark AM allocated vcores
  spark.yarn.am.cores: 1
  # Default spark AM allocated memory
  spark.yarn.am.memory: 640m
  # spark.yarn.am.memoryOverhead can be needed, default value: AM memory * 0.10, with minimum of 384
  # Default spark driver allocated vcores
  spark.driver.cores: 1
  # Default spark driver allocated memory
  spark.driver.memory: 640m
  # spark.driver.memoryOverhead can be needed, default value: driver memory * 0.10, with minimum of 384
  # Default spark executor allocated vcores
  spark.executor.cores: 1
  # Default spark executor allocated memory
  spark.executor.memory: 640m
  # spark.executor.memoryOverhead can be needed, default value: executor memory * 0.10, with minimum of 384

# spark-defaults.conf - Spark History Server
spark_defaults_hs:
  spark.history.kerberos.enabled: "true"
  spark.history.kerberos.keytab: /etc/security/keytabs/spark.service.keytab
  spark.history.kerberos.principal: "spark/{{ ansible_fqdn }}@{{ realm }}"
  spark.history.ui.acls.enable: "true"
  spark.history.ui.admin.acls: "knox,hue"
  spark.history.ui.port: "{{ spark3_hs_http_port }}"
  spark.ui.filters: org.apache.hadoop.security.authentication.server.AuthenticationFilter
  spark.org.apache.hadoop.security.authentication.server.AuthenticationFilter.params: type=kerberos,kerberos.principal={{ spark_ui_spnego_principal }},kerberos.keytab={{ spark_ui_spnego_keytab }},cookie.domain={{ http_cookie_domain }},signature.secret.file={{ http_secret_location }}
  spark.ssl.historyServer.enabled: "true"
  spark.ssl.historyServer.keyStore: "{{ spark_keystore_location }}"
  spark.ssl.historyServer.port: "{{ spark3_hs_https_port }}"
  spark.history.fs.logDirectory: "hdfs://{{ cluster_name }}/spark3-logs"
  spark.history.provider: org.apache.spark.deploy.history.FsHistoryProvider
  spark.ui.proxyBase: "{% if 'knox' in groups and groups['knox'] %}/gateway/tdpldap/spark3history{% endif %}"
  spark.hadoop.hadoop.security.credential.provider.path: localjceks://file{{ spark_hs_conf_dir }}/{{ hadoop_credentials_store_file }}

# spark-defaults.conf - Spark Iceberg added when `spark_iceberg_enable` is true
spark_defaults_iceberg:
  spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
  # Spark catalog 'iceberg' configured only for Iceberg operations
  spark.sql.catalog.iceberg: org.apache.iceberg.spark.SparkCatalog
  spark.sql.catalog.iceberg.type: hive
  # The properties below replicate the properties when creating an Iceberg table with `CREATE TABLE` in Beeline
  # spark.sql.catalog.iceberg.table-override.external.table.purge: true
  # spark.sql.catalog.iceberg.table-override.iceberg.orc.files.only: false

# spark-env.sh - common
spark_env_common:
  PYSPARK_PYTHON: python3
  HADOOP_CONF_DIR: "{{ hadoop_conf_dir }}"
  HADOOP_HOME: "{{ hadoop_home }}"
  HIVE_CONF_DIR: "{{ spark_client_conf_dir }}"
  LD_LIBRARY_PATH: "'{{ hadoop_home }}/lib/native/:$LD_LIBRARY_PATH'"

# spark-env.sh - Spark History Server
spark_env_hs:
  SPARK_LOG_DIR: "{{ spark_log_dir }}"
  SPARK_DAEMON_JAVA_OPTS: "'{{ jmx_common_opts }} {{ jmx_exporter_hs_opts }}'"
  SPARK_DAEMON_MEMORY: "{{ spark3_hs_heapsize }}"

# spark-env.sh - Spark Client
spark_env_client:

# hive-site.xml - spark client
hive_site_spark:
  hive.metastore.uris: |-
    {{ groups['hive_ms'] |
      map('tosit.tdp.access_fqdn', hostvars) |
      map('regex_replace', '^', 'thrift://')|
      map('regex_replace', '^(.*)$', '\1:' + hive_metastore_listener_port | string) |
      list |
      join(',') }}
  metastore.thrift.uri.selection: RANDOM
  hive.exec.scratchdir: /tmp/spark
  hive.metastore.client.connect.retry.delay: 5
  hive.metastore.client.socket.timeout: 1800
  hive.server2.enable.doAs: "false"
  hive.server2.transport.mode: http
  hive.server2.thrift.port: "{{ hive_hiveserver2_thrift_port }}"
  hive.server2.thrift.http.port: "{{ hive_hiveserver2_thrift_http_port }}"
  hive.metastore.sasl.enabled: "true"
  hadoop.security.authentication: kerberos
  hive.metastore.kerberos.principal: "hive/_HOST@{{ realm }}"
  hive.metastore.execute.setugi: "true"
  hadoop.rpc.protection: AUTHENTICATION
  hive.execution.engine: spark

# hive-site.xml - Spark Iceberg  added when `spark_iceberg_enable` is true
hive_site_iceberg:
  # Configure Spark Iceberg to use Hive compatible Serde and Format
  iceberg.engine.hive.enabled: "true"

# Spark3 hs resources allocation
spark3_hs_heapsize: 1024m

# jmx exporter configuration file
spark_jmx_exporter_conf_file: jmx-exporter.yml

# JMX exporter properties
jmx_exporter_hs_opts: "{% if 'exporter_jmx' in groups and groups['exporter_jmx'] %}-javaagent:{{ jmx_exporter_install_file }}={{ exporter_spark3_hs_http_port }}:{{ spark_conf_dir }}/{{ spark_jmx_exporter_conf_file}}{% endif %}"
jmx_exporter:
  startDelaySeconds: 0
  lowercaseOutputName: false
  lowercaseOutputLabelNames: false
  httpServer:
    authentication:
      basic:
        username: "{{ exporter_spark3_jmxremote_username }}"
        password: "{{ exporter_spark3_jmxremote_password }}"
    ssl:
      keyStore:
        filename: "{{ spark_keystore_location }}"
        password: "{{ spark_keystore_password }}"
      certificate:
        alias: "{{ ansible_fqdn }}"

# Service start on boot policies
spark_hs_start_on_boot: no

# Service restart policies
spark_hs_restart: "no"
